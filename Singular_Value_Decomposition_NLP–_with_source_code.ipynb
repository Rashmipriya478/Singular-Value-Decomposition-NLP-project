{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#We will see that how we can perform Singular Value Decomposition of some book titles we are having in our dataset using TruncatedSVD.\n",
        "This transformer performs linear dimensionality reduction by means of truncated singular value decomposition (SVD).\n",
        "\n",
        "Contrary to PCA, this estimator does not center the data before computing the singular value decomposition. This means it can work with sparse matrices efficiently. When we perform SVD (Singular Value Decomposition) on text data it is also called LSA (Latent Semantic Analysis)."
      ],
      "metadata": {
        "id": "54Ug1QFJkA0L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Step 1 – Importing libraries required for Singular Value Decomposition."
      ],
      "metadata": {
        "id": "6c6i_2tpkFQH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import numpy as np\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "%matplotlib inline\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "id": "JxcBUcN0kOT4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 2 – Reading lines from our text file."
      ],
      "metadata": {
        "id": "ijqZeLNPkTzp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "titles = [line.strip() for line in open('/content/all_book_titles (1).txt')]\n",
        "titles"
      ],
      "metadata": {
        "id": "5WbnaQ4CkWUF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 3 – Creating a Stopwords set."
      ],
      "metadata": {
        "id": "UIKmFSoElIif"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "stopwords = set(word.strip() for word in open('/content/stopwords (1).txt'))\n",
        "stopwords = stopwords.union({\n",
        "    'introduction', 'edition', 'series', 'application',\n",
        "    'approach', 'card', 'access', 'package', 'plus', 'etext',\n",
        "    'brief', 'vol', 'fundamental', 'guide', 'essential', 'printed',\n",
        "    'third', 'second', 'fourth', })\n",
        "\n",
        "word_lemmatizer = WordNetLemmatizer()"
      ],
      "metadata": {
        "id": "woaaadXGlDFp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "We have our default stopwords like a, is, that, this, etc. in stopwords.txt. So first of all we are loading all those in a variable called stopwords.\n",
        "Secondly, we are adding some more stopwords like edition, introduction, series, etc. manually in the stopwords set. These are some very common words that occur in Book Titles.\n",
        "Lastly we are initializing WordNetLemmatizer()."
      ],
      "metadata": {
        "id": "phBUn5PylXNb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 4 – Creating tokenizer function."
      ],
      "metadata": {
        "id": "BBnbdA6GlZkE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenizer(s):\n",
        "    s = s.lower()\n",
        "    tokens = nltk.tokenize.word_tokenize(s)\n",
        "    tokens = [t for t in tokens if len(t)>2]\n",
        "    tokens = [word_lemmatizer.lemmatize(t) for t in tokens]\n",
        "    tokens = [t for t in tokens if t not in stopwords]\n",
        "    tokens = [t for t in tokens if not any(c.isdigit() for c in t)]\n",
        "    return tokens"
      ],
      "metadata": {
        "id": "eHWt7ivMldFU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This will take a string, convert it to lowercase, tokenize it, remove words having a length less than 2, lemmatize words, remove stopwords and finally remove all those words having any digit in them."
      ],
      "metadata": {
        "id": "M-YliYhblf_W"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 5 – Checking tokenizer."
      ],
      "metadata": {
        "id": "6Fs2vhjElkbt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer('my name is abhishek and i am 19 years old!!')"
      ],
      "metadata": {
        "id": "YI9WgLfQlnER"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 6 – Creating word_2_int and int_2_word dictionaries."
      ],
      "metadata": {
        "id": "uwBu1l84lp2l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "word_2_int = {}\n",
        "int_2_words = {}\n",
        "ind = 0\n",
        "error_count = 0\n",
        "\n",
        "for title in titles:\n",
        "    try:\n",
        "        title = title.encode('ascii', 'ignore').decode('utf-8') # this will throw exception if bad characters\n",
        "        tokens = tokenizer(title)\n",
        "        for token in tokens:\n",
        "            if token not in word_2_int:\n",
        "                word_2_int[token] = ind\n",
        "                int_2_words[ind]=token\n",
        "                ind += 1\n",
        "    except Exception as e:\n",
        "        print(e)\n",
        "        print(title)\n",
        "        error_count += 1"
      ],
      "metadata": {
        "id": "JHIJ9rw9lwhF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We have used try-except because there could be some titles that have some special characters. Those will throw exceptions.\n",
        "Then we simply take the title and tokenize it and then simply traverse in those tokens.\n",
        "If the token is not in our vocabulary (word_2_int), append it and give a token number to it.\n",
        "Similarly, create an apposite dictionary int_2_word also for future use.\n",
        "And then simply increment the index.\n"
      ],
      "metadata": {
        "id": "yXALYdMXlzXG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 7 – Creating tokens_2_vectors function."
      ],
      "metadata": {
        "id": "DmK7qatUl2Jx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def tokens_2_vectors(tokens):\n",
        "    X = np.zeros(len(word_2_int))\n",
        "    for t in tokens:\n",
        "        try:\n",
        "            index = word_2_int[t]\n",
        "            X[index]=1\n",
        "        except:\n",
        "            pass\n",
        "    return X"
      ],
      "metadata": {
        "id": "SiEHgstcl6Yg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This function simply converts a title in the token form to a vector.\n",
        "It will create an array of our vocabulary size with all elements as 0.\n",
        "It will then replace 0s with 1s for the words that are present in the title whose vector we are creating."
      ],
      "metadata": {
        "id": "KCyl5dSal9u6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 8 – Creating a final matrix and fitting it into our SVD."
      ],
      "metadata": {
        "id": "32Zh9VNQmAQc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "final_matrix = np.zeros((len(titles),len(word_2_int)))\n",
        "\n",
        "for i in range(len(titles)):\n",
        "    title = titles[i]\n",
        "    token = tokenizer(title)\n",
        "    final_matrix[i,:] = tokens_2_vectors(token)\n",
        "\n",
        "svd = TruncatedSVD()\n",
        "Z = svd.fit_transform(final_matrix)\n",
        "Z.shape"
      ],
      "metadata": {
        "id": "G5nuXUubmCXi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we are creating a final matrix with rows as no of titles (2373) and columns as no of words in our vocabulary.\n",
        "Then we are filling this matrix with vectors of each and every title.\n",
        "Finally fitting our TruncatedSVD with our final matrix.\n",
        "It means that our data is having 2373 titles and 2 represents the position of that token in the plane."
      ],
      "metadata": {
        "id": "q6D52tbumGAK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 9 – Visualize the results."
      ],
      "metadata": {
        "id": "JmLOOz0QmI1v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fig = plt.figure(figsize=(15,9))\n",
        "plt.scatter(Z[:,0],Z[:,1])\n",
        "for i in range(len(word_2_int)):\n",
        "    plt.annotate(int_2_words[i],(Z[i,0],Z[i,1]))"
      ],
      "metadata": {
        "id": "J1IZRBZfmK4R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You will see that similar words will be closer in this plot."
      ],
      "metadata": {
        "id": "1x0yGDLAmUn9"
      }
    }
  ]
}